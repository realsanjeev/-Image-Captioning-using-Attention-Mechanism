{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/realsanjeev/image-captioning-using-machine-learning/blob/main/image_captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "0YvF62bv07FR",
        "outputId": "4a84b4cb-4245-46b6-cd8a-98648e0e9cff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python 3.10.12 (main, Jun  7 2023, 12:45:35) [GCC 9.4.0]\n",
            "Numpy version: 1.22.4\n",
            "padas vesion: 1.5.3\n",
            "keras version: 2.12.0\n",
            "tensorflow version: 2.12.0\n",
            "tensorflow datasets version: 4.9.2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-56f52743b6d2>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tensorflow version: {tf.__version__}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tensorflow datasets version: {tfds.__version__}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tensorflow hub version: {hub.__version__}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'hub' is not defined"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import time\n",
        "import os\n",
        "import warnings\n",
        "# for doing operation in gpu\n",
        "import numpy as np\n",
        "# for working in gpu numpy equivalent\n",
        "# import cupy\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from collections import Counter\n",
        "from tensorflow.python.keras.backend import set_session\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "print(f\"python {sys.version}\")\n",
        "print(f\"Numpy version: {np.__version__}\")\n",
        "# print(f\"cupy version: {np.__version__}\")\n",
        "print(f\"padas vesion: {pd.__version__}\")\n",
        "print(f\"keras version: {tf.keras.__version__}\");\n",
        "print(f\"tensorflow version: {tf.__version__}\")\n",
        "print(f\"tensorflow datasets version: {tfds.__version__}\")\n",
        "print(f\"tensorflow hub version: {hub.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxuqBeU74G2J"
      },
      "outputs": [],
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "print(\"Num GPUs Available: \", len(gpus))\n",
        "# Limiting GPU memory growth\n",
        "if gpus:\n",
        "  try:\n",
        "    # Currently, memory growth needs to be the same across GPUs\n",
        "    for gpu in gpus:\n",
        "      tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "  except RuntimeError as e:\n",
        "    # Memory growth must be set before GPUs have been initialized\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzL7V1nl7X5w"
      },
      "outputs": [],
      "source": [
        "# is cuda available\n",
        "print(tf.test.is_built_with_cuda())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset loading\n",
        "Dataset is loaded in cloud rather than ocal machine. Since it is more tedious to dowload the large data and process in local machine. The data is loaded from google cloud system"
      ],
      "metadata": {
        "id": "DViX-kS99oBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 1000\n",
        "GCS_DIR = \"gs://asl-public/data/tensorflow_datasets/\"\n",
        "\n",
        "trainds = tfds.load(\"coco_captions\", split=\"train\", data_dir= GCS_DIR)\n",
        "sample_record = trainds.take(1)\n",
        "print(sample_record)"
      ],
      "metadata": {
        "id": "GeDmhCk6N8O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_HEIGHT = 299\n",
        "IMG_WIDTH = 299\n",
        "N_CHANNELS = 3\n",
        "TARGET_SIZE = (IMG_HEIGHT, IMG_WIDTH, N_CHANNELS)"
      ],
      "metadata": {
        "id": "YB8yK6M-MMxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_with_caption(record):\n",
        "  # get only one caption\n",
        "  caption = record['captions'][\"text\"][0]\n",
        "  img = record['image']\n",
        "  img = tf.image.resize(img, (IMG_HEIGHT, IMG_WIDTH))\n",
        "  img = img / 255\n",
        "  return {\"image_tensor\": img, \"caption\": caption}\n",
        "\n",
        "trainds = trainds.map(\n",
        "  get_image_with_caption, num_parallel_calls=tf.data.AUTOTUNE\n",
        ").shuffle(BUFFER_SIZE)\n",
        "\n",
        "trainds = trainds.prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "dxNiw4jcTPJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainds.element_spec"
      ],
      "metadata": {
        "id": "zKTMus2aBj7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization"
      ],
      "metadata": {
        "id": "4yfsTH4mCUmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "npic = 5 # Displaying 5 images from the dataset\n",
        "\n",
        "count = 1\n",
        "fig = plt.figure(figsize=(10,20))\n",
        "for record in trainds.take(npic):\n",
        "  caption = record[\"caption\"]\n",
        "  image = record[\"image_tensor\"]\n",
        "  ax = fig.add_subplot(npic, 2, count, xticks=[], yticks=[])\n",
        "  ax.imshow(image)\n",
        "  count += 1\n",
        "\n",
        "  ax = fig.add_subplot(npic, 2, count)\n",
        "  plt.axis(False)\n",
        "  ax.plot()\n",
        "  ax.set_xlim(0,1)\n",
        "  ax.set_ylim(0,len(caption.numpy()))\n",
        "  # for i, caption in enumerate(caption):\n",
        "  ax.text(0, 1, caption.numpy(), fontsize=20)\n",
        "  count += 1\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EBlWjKz_ZtXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_start_end_token(data):\n",
        "  start = tf.convert_to_tensor(\"startseq\")\n",
        "  end = tf.convert_to_tensor(\"endseq\")\n",
        "  data[\"caption\"] = tf.strings.join([start, data[\"caption\"], end],\n",
        "                                    separator=\" \")\n",
        "  return data\n",
        "\n",
        "trainds = trainds.map(add_start_end_token)"
      ],
      "metadata": {
        "id": "qPI3XhtzCwXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_CAPTION_LEN = 65\n",
        "VOCAB_SIZE = 10000\n",
        "\n",
        "def standardize(caption):\n",
        "  caption = tf.strings.lower(caption)\n",
        "  return tf.strings.regex_replace(\n",
        "      caption, r\"[!\\\"#$%&\\(\\)\\*\\+.,-/:;=?@\\[\\\\\\]^_`{|}~]?\", \"\"\n",
        "  )\n",
        "\n",
        "# Use textvectorization\n",
        "tokenizer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    standardize=standardize,\n",
        "    output_sequence_length=MAX_CAPTION_LEN\n",
        ")\n",
        "\n",
        "tokenizer.adapt(trainds.map(lambda x: x[\"caption\"]))\n",
        "print(\"finished\")"
      ],
      "metadata": {
        "id": "Iiaaje69HZfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word_id in tokenizer([\"startseq This is me from future colab using token endseq\"])[0]:\n",
        "  print(tokenizer.get_vocabulary()[word_id], end=\" \")"
      ],
      "metadata": {
        "id": "sS4_4LTLKAuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, data in enumerate(trainds.take(5)):\n",
        "  caption = data[\"caption\"]\n",
        "  print(f\"Sample Sentence {i}: {caption.numpy()}\")\n",
        "  print(f\"Equivalent tokenized tensor {i}: {tokenizer(caption)}\")"
      ],
      "metadata": {
        "id": "cw6A3NGeS4J6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5sUx8uBNBKjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Original sentence: {caption}\")\n",
        "print(\"Reconstructed sentence: \")\n",
        "for word_id in tokenizer(caption).numpy():\n",
        "  print(tokenizer.get_vocabulary()[word_id], end=\" \")"
      ],
      "metadata": {
        "id": "0lk38xbfA1ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# string lookup word to id\n",
        "word_to_idx = tf.keras.layers.StringLookup(\n",
        "    mask_token=\"\", vocabulary=tokenizer.get_vocabulary()\n",
        ")\n",
        "# word_id to string\n",
        "idx_to_word = tf.keras.layers.StringLookup(\n",
        "    mask_token=\"\", vocabulary=tokenizer.get_vocabulary(), invert=True\n",
        ")"
      ],
      "metadata": {
        "id": "omlrqGFlCEUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For training module to predict caption\n",
        "To train a module for caption prediction, the tensor is shifted from right to left while maintaining the same target shape.\n",
        "\n",
        "A simple analogy for this process is predicting the next word based on the previous words."
      ],
      "metadata": {
        "id": "EYYEK5J9ItY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "def create_ds_fn(data):\n",
        "  image_tensor = data[\"image_tensor\"]\n",
        "  caption = tokenizer(data[\"caption\"])\n",
        "\n",
        "  # shift tensor right to left eg, [1, 2,3] => [2, 3]\n",
        "  target = tf.roll(caption, -1, 0)\n",
        "  zeros = tf.zeros([1], dtype=tf.int64)\n",
        "  # mantain shape of target by adding zero element in last index\n",
        "  target = tf.concat((target[:-1], zeros), axis=-1)\n",
        "  return (image_tensor, caption), target\n",
        "\n",
        "batched_ds = (\n",
        "    trainds.map(create_ds_fn)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "fPWGagbkD1sE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extractor\n",
        "We use pretained model inception_resnet_v2 for extracting feature"
      ],
      "metadata": {
        "id": "O1BizS0PFX3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "FEATURE_EXTRACTOR = tf.keras.applications.inception_resnet_v2.InceptionResNetV2(\n",
        "    include_top=False, weights=\"imagenet\"\n",
        ")\n",
        "FEATURES_SHAPE = (8, 8, 1536)\n",
        "FEATURE_EXTRACTOR.trainable = False\n",
        "ATTENTION_DIM = 512\n",
        "\n",
        "image_input = tf.keras.layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, N_CHANNELS))\n",
        "image_features = FEATURE_EXTRACTOR(image_input)\n",
        "\n",
        "x = tf.keras.layers.Reshape((FEATURES_SHAPE[0]*FEATURES_SHAPE[1], FEATURES_SHAPE[2]))(image_features)\n",
        "encoder_output = tf.keras.layers.Dense(ATTENTION_DIM, activation=\"relu\")(x)\n",
        "\n",
        "encoder = tf.keras.Model(inputs=image_input,\n",
        "                         outputs=encoder_output)\n",
        "encoder.summary()"
      ],
      "metadata": {
        "id": "S47v2ZSbGa31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_input = tf.keras.layers.Input(shape=(MAX_CAPTION_LEN), name=\"words\")\n",
        "embed_x = tf.keras.layers.Embedding(VOCAB_SIZE, ATTENTION_DIM)(word_input)\n",
        "\n",
        "decoder_gru = tf.keras.layers.GRU(\n",
        "    ATTENTION_DIM,\n",
        "    return_sequences=True,\n",
        "    return_state=True,\n",
        ")\n",
        "gru_output, gru_state = decoder_gru(embed_x)\n",
        "\n",
        "decoder_attention = tf.keras.layers.Attention()\n",
        "context_vector = decoder_attention([gru_output, encoder_output])\n",
        "\n",
        "addition = tf.keras.layers.Add()([gru_output, context_vector])\n",
        "\n",
        "layer_norm = tf.keras.layers.LayerNormalization(axis=-1)\n",
        "layer_norm_out = layer_norm(addition)\n",
        "\n",
        "decoder_output_dense = tf.keras.layers.Dense(VOCAB_SIZE)\n",
        "decoder_output = decoder_output_dense(layer_norm_out)\n",
        "\n",
        "decoder = tf.keras.Model(\n",
        "    inputs=[word_input, encoder_output],\n",
        "    outputs=decoder_output\n",
        ")\n",
        "decoder.summary()"
      ],
      "metadata": {
        "id": "jj4lDE-8RYU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.plot_model(decoder)"
      ],
      "metadata": {
        "id": "AyXIqVxORYpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_caption_training_model = tf.keras.Model(\n",
        "    inputs=[image_input, word_input], outputs=decoder_output\n",
        ")\n",
        "tf.keras.utils.plot_model(image_caption_training_model)"
      ],
      "metadata": {
        "id": "XuXVL6D3ExYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining loss function and compile"
      ],
      "metadata": {
        "id": "wjkwxx5EF5aD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_module = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction=\"none\")\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  loss_ = loss_module(real, pred)\n",
        "\n",
        "  mask=tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  mask = tf.cast(mask, dtype=tf.int32)\n",
        "  sentence_len = tf.reduce_sum(mask)\n",
        "  loss_ = loss_[:sentence_len]\n",
        "\n",
        "  return tf.reduce_mean(loss_, 1)"
      ],
      "metadata": {
        "id": "DenqvgY2zmVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_caption_training_model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=loss_function\n",
        ")"
      ],
      "metadata": {
        "id": "M-bQK452GAMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wall time vs CPU time\n",
        "> CPU time - the time actually spent by CPU executing method code.\n",
        "\n",
        "> Wall time - the real-world time elapsed between a pair of events, e.g. between method entry and method exit"
      ],
      "metadata": {
        "id": "EfR4bMJZ9OUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history = image_caption_training_model.fit(batched_ds, epochs=1)"
      ],
      "metadata": {
        "id": "vwAFi6Ue7OJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Caption Prediction"
      ],
      "metadata": {
        "id": "38P7GpcQ6fz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gru_state_input = tf.keras.layers.Input(\n",
        "    shape=ATTENTION_DIM,\n",
        "    name=\"GRU attention input\"\n",
        ")\n",
        "# reusing the trained gru but update to receive the state\n",
        "gru_output, gru_state = decoder_gru(\n",
        "    embeded_x,\n",
        "    initial_state=gru_state_input)\n",
        "\n",
        "# reusing other layer\n",
        "contex_vector=decoder_attention([gru_output, encoder_output])\n",
        "addition_output = tf.keras.layer.Add()([\n",
        "    gru_output, contex_vector\n",
        "])\n",
        "layer_norm_output = decoder_output_dense(layer_norm_output)\n",
        "\n",
        "# defining the prediction model\n",
        "decoder_pred_model = tf.keras.Model(\n",
        "    inputs=[word_input, gru_state_input, encoder_output],\n",
        "    outputs=[decoder_output, gru_state]\n",
        ")\n",
        "tf.keras.utils.plot_model(decoder_pred_output)"
      ],
      "metadata": {
        "id": "F9wVlIwd6c4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predicting caption using trained model\n",
        "def predict_caption(filename):\n",
        "  gru_state = tf.zeros(1, ATTENTION_DIMS)\n",
        "\n",
        "  img_tensor = tf.image.decode_jpeg(tf.io.read_file(filename))\n",
        "  img_tensor = tf.image.resize(img_tensor, (IMG_HEIGHT, IMG_WIDTH))\n",
        "  img_tensor = img_tensor / 255.0\n",
        "\n",
        "\n",
        "\n",
        "  features = encoder(tf.expand_dims(img, axis=0))\n",
        "  dec_input = tf.expand_dims([word_to_idx(\"startseq\")], 1)\n",
        "  results = []\n",
        "  for i in range(MAX_CAPTION_LEN):\n",
        "    predictations, gru_state = decoder_pred_model(\n",
        "        [dec_input, gru_state, features]\n",
        "    )\n",
        "\n",
        "    # probabilistic distribution\n",
        "    top_probs, top_idxs = tf.math.top_k(\n",
        "        input=predictions[0][0], k=10, sorted=False\n",
        "    )\n",
        "    chosen_id = tf.random.categorical([top_probs], 1)\n",
        "    predicted_id = top_idxs.numpy()[chosen_id][, ]\n",
        "\n",
        "    result.append(tokenizer.get_vocabulary()[prediccted_id])\n",
        "    if predicted_id == word_to_idx(\"endseq\"):\n",
        "      return img, result\n",
        "\n",
        "    dec_input = tf.expand_dims([prediccted_id], 1)\n",
        "  return img, result"
      ],
      "metadata": {
        "id": "Bu6kkXmv6kWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"../sample_images/baseball.jpeg\"  # you can also try surf.jpeg\n",
        "\n",
        "for i in range(5):\n",
        "  image, caption = predict_caption(filename)\n",
        "  print(\" \".join(caption[:-1]) + \".\")\n",
        "\n",
        "img = tf.image.decode_jpeg(tf.io.read_file(filename), channels=IMG_CHANNELS)\n",
        "plt.imshow(img)\n",
        "plt.axis(\"off\");"
      ],
      "metadata": {
        "id": "sp87AUgq9aSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-IUktCp69azn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}